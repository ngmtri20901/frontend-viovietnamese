
# Building a Custom UI from LLM-Generated JSON Data with Vercel AI SDK

This is how to create a **Custom UI** based on **JSON** data generated by a **Large Language Model** (**LLM**) within a **chat** conversation, using the **Vercel AI SDK UI/Core**. The most effective methods for this are through the **Generative UI** feature or **Streaming Custom Data** 

The **AI SDK UI** is a **toolkit** designed to help you build **chat** applications, text completion, and interactive assistants, simplifying the management of the **chat stream** and **UI** updates on the **frontend**.

-----

## 1\. Core Mechanism: UIMessage and Data Parts

In **AI SDK 5**, chat history is explicitly managed through two message types:

1.  **ModelMessage**: A streamlined representation, optimized for sending to **Large Language Models** (**LLM**s).
2.  **UIMessage**: The **source of truth** for your application state, containing all messages, **metadata**, **tool** results, and other data parts. This is the recommended format for maintaining the visible **chat** history for the user.

To create a **Custom UI** from **JSON**, you will focus on customizing and utilizing the **parts** array of the **UIMessage**. This **parts** array contains the different content components of a message, including text, **tool** results, and **data parts** (custom data parts).

You can customize the **UIMessage** to create your own data type with the exact shape of data, **tools**, and **metadata** appropriate for your application, ensuring **full-stack type-safety**.

-----

## 2\. The Generative UI Method (Using Tools to Return JSON)

This is the most common way for the **LLM** to trigger complex **UI components** by forcing the model to call a "**tool**," and the result of that **tool** is the **JSON** data used to **render the component**.

### A. The Backend (API Route)

You need to define a **Tool** with an **execute** function that returns the desired **JSON** data:

1.  **Define Tool and Schema**: Use the **tool** function and define the **inputSchema** (e.g., with **Zod**) for the parameters the **LLM** needs to call that **tool**.
2.  **Execute the Tool and Return JSON**: In the **tool**'s **execute** function, instead of returning text, you return a **JSON object** (the **props** data for the **UI component**).

**Example (Based on weatherTool):**

```typescript
// ai/tools.ts
import { tool as createTool } from 'ai';
import { z } from 'zod';

export const weatherTool = createTool({
    description: 'Display the weather for a location',
    inputSchema: z.object({
        location: z.string().describe('The location to get the weather for'),
    }),
    execute: async function ({ location }) {
        // ... (API call or business logic)
        return { weather: 'Sunny', temperature: 75, location }; // Trả về JSON (Return JSON)
    },
});

export const tools = {
    displayWeather: weatherTool, // Tên tool là displayWeather (The tool name is displayWeather)
};
```

3.  **Use streamText and toUIMessageStreamResponse**: In the **Route Handler** (e.g., `app/api/chat/route.ts`), you use **streamText** with the list of defined **tools**. The **AI SDK** automatically handles running the **tool** and wrapping the **JSON** result into a **Tool** element within the **UIMessage stream**.

<!-- end list -->

```typescript
// app/api/chat/route.ts
import { streamText, convertToModelMessages } from 'ai';
import { openai } from '@ai-sdk/openai';
// Import tools đã định nghĩa (Import defined tools)
import { tools } from '@/ai/tools'; 

export async function POST(request: Request) {
    const { messages } = await request.json();
    const result = streamText({
        model: openai('gpt-4o'),
        messages: convertToModelMessages(messages),
        tools, // Truyền tool vào (Pass the tool)
    });
    return result.toUIMessageStreamResponse(); // Trả về luồng UIMessage (Return the UIMessage stream)
}
```

### B. The Frontend (Client Component)

You use the **useChat hook** and **render the component** based on the **parts** elements of the message:

1.  **Use useChat**: This **hook** manages the message state (**messages**) and receives updates from the **UIMessage stream**.
2.  **Render Custom UI**: Iterate through the `message.parts` array. When you encounter an element with a **type** of `tool-{toolName}` (e.g., `tool-displayWeather`), you check the **state** and **render your custom React component**, passing the **JSON** data from `part.output` as **props**.

**Example of UI rendering logic in the client component:**

```jsx
// Render component Weather tùy chỉnh (Render custom Weather component)
{messages.map(message => (
    <div key={message.id}>
        {/* ... */}
        {message.parts.map((part, index) => {
            if (part.type === 'tool-displayWeather') {
                switch (part.state) {
                    case 'input-available': // Tool has input, waiting for result
                        return <div key={index}> Loading weather ... </div>; 
                    case 'output-available': // Tool has finished running, has JSON output
                        return (
                            <div key={index}>
                                {/* Truyền JSON (part.output) làm props (Pass JSON (part.output) as props) */}
                                <Weather {...part.output} /> 
                            </div>
                        ); 
                    case 'output-error':
                        return <div key={index}> Error: {part.errorText} </div>; 
                    // Các trạng thái khác (ví dụ: 'input-streaming') (Other states (e.g.: 'input-streaming'))
                    default:
                        return null;
                }
            }
            return null;
        })}
    </div>
))}
```

-----

## 3\. The Streaming Custom Data Method (Data Parts)

If you want to stream custom **JSON** data that is not the result of a **tool** call (e.g., status updates, **RAG** source), you can use **Data Parts**.

### A. The Backend (Using `createUIMessageStream`)

You can use **createUIMessageStream** to create a custom **UI** data stream and send "**data parts**" containing **JSON**.

1.  **Customize UIMessage (Type Safety)**: Define the `MyUIMessage` data type to include your custom data type (e.g., `MyDataParts`).
2.  **Send Data Parts**: Within **createUIMessageStream**, you use `writer.write()` to send data parts:
      * Each **data part** has a **type** (starting with `data-`) and a **data** field containing your **JSON payload**.
      * If you provide an **id**, you can update that same data element later (**Data Part Reconciliation**).

**Example of sending a loading status and then the final weather result:**

```typescript
// On the server
const stream = createUIMessageStream < MyUIMessage > ({
    execute: ({ writer }) => {
        const dataPartId = 'weather-1';
        // 1. Gửi trạng thái tải ban đầu (Send initial loading status)
        writer.write({
            type: 'data-weather', // Type-checked against MyUIMessage
            id: dataPartId,
            data: { city: 'San Francisco', status: 'loading' },
        }); 

        // ... (Tiến hành gọi LLM hoặc logic kinh doanh) (Proceed with LLM call or business logic)

        // 2. Cập nhật cùng một phần tử với kết quả cuối cùng (JSON) (Update the same element with the final result (JSON))
        writer.write({
            type: 'data-weather',
            id: dataPartId, // Cùng ID để cập nhật (Same ID for update)
            data: { city: 'San Francisco', weather: 'sunny', status: 'success' },
        }); 
    }
});
return createUIMessageStreamResponse({ stream }); 
```

### B. The Frontend (Handling Data Parts)

**Data Parts** (which are not **transient**) will be added to the `message.parts` array of the **UIMessage**.

Use the **useChat hook** and iterate through the `message.parts` array to **render the custom UI** when encountering `data-weather`:

```jsx
// On the client
const { messages } = useChat < MyUIMessage > ();

{messages.map(message => (
    <div key={message.id}>
        {message.parts
            .filter(part => part.type === 'data-weather') // Lọc ra Data Parts của bạn (Filter your Data Parts)
            .map((part, index) => (
                <span key={index} className="weather-update">
                    {part.data.status === 'loading' ? (
                        <> Getting weather for {part.data.city} ... </>
                    ) : (
                        // Render UI tùy chỉnh dựa trên JSON trong part.data (Render custom UI based on JSON in part.data)
                        <WeatherComponent data={part.data} /> // Ví dụ: sử dụng component React (Example: using a React component)
                    )}
                </span>
            ))
        }
        {/* ... render các phần khác (text) (... render other parts (text)) */}
    </div>
))}
```

Both methods (**Generative UI** using **Tools** and **Streaming Data Parts**) allow you to transfer structured data (**JSON**) from the **backend** to the **frontend** in a **type-safe** manner and use that data to create custom **UI components** within the **chat stream**.


## Step-by-Step Process for Creating a Tool in AI SDK (with Generative UI Integration)

This process explains how to create a **tool** in the AI SDK that allows the LLM to return **tool parts** data, which are then passed into **UI Components** to generate a **Generative UI**, focusing on **AI SDK UI** with `streamText`.

---

### Workflow Overview (Using AI SDK UI)

The entire Generative UI creation workflow via Tool Calling is divided into three main parts: **Tool Definition**, **Server-side Processing**, and **Client-side Rendering**.

---

### Step 1: Define the Tool (Server-side File: `ai/tools.ts`)

You need a file to define the tool and its execution logic.

1. **Use `tool()`:** The helper `tool()` (from AI SDK Core) safely creates a tool with type-safety.
2. **Set `description`:** Provide a description to guide the LLM on when to call this tool.
3. **Set `inputSchema`:** Define the input parameters (with Zod Schema or JSON Schema) that the LLM must generate to call the tool.
4. **Define `execute`:** An async function that executes the tool’s logic. It takes `input` and returns the result (data).

   * **The returned result from `execute` will be the data passed into the UI Component later.**

**Example Tool File (`ai/tools.ts`)**:

```typescript
// ai/tools.ts
import { tool } from 'ai';
import { z } from 'zod'; // Zod Schema

export const weatherTool = tool({
  description: 'Get the current weather information for a location.',
  inputSchema: z.object({
    location: z.string().describe('The city and state'),
  }),
  execute: async ({ location }) => {
    // Simulated API call
    await new Promise(resolve => setTimeout(resolve, 2000));
    return {
      location,
      temperature: 72 + Math.floor(Math.random() * 21) - 10,
      weather: 'sunny'
    }; // This result becomes the Tool Part output
  },
});

export const tools = {
  displayWeather: weatherTool, // Tool name definition
};
```

---

### Step 2: Configure and Process on Server (Route Handler: `app/api/chat/route.ts`)

The Route Handler (e.g., in Next.js App Router) is where the LLM is invoked, tools are provided, and the UI response stream is created.

1. **Receive Client Messages:** Accept `UIMessage[]` from the client.
2. **Convert Messages:** Convert `UIMessage[]` to `ModelMessage[]` using `convertToModelMessages`.
3. **Call `streamText`:** Use `streamText` (AI SDK Core) and pass the defined tools.

   * Use `stopWhen: stepCountIs(N)` to enable multi-step tool calls.
4. **Create UI Stream:** Convert LLM streaming results to an HTTP stream (`Server-Sent Events`) via `result.toUIMessageStreamResponse()`, which `useChat` on the client can read.

**Example Route Handler (`app/api/chat/route.ts`)**:

```typescript
import { openai } from '@ai-sdk/openai';
import { streamText, convertToModelMessages, UIMessage, stepCountIs } from 'ai';
import { tools } from '@/ai/tools'; // Import defined tools

export async function POST(request: Request) {
  const { messages }: { messages: UIMessage[] } = await request.json();

  const result = streamText({
    model: openai('gpt-4o'),
    messages: convertToModelMessages(messages),
    stopWhen: stepCountIs(5), // Allow multi-step execution
    tools,
  });

  return result.toUIMessageStreamResponse(); // Return UI Message stream
}
```

---

### Step 3: LLM Returns Tool Part Data

When the LLM decides to use a tool (e.g., user asks: “What’s the weather in San Francisco?”), the server-side flow is:

1. LLM generates a **Tool Call** with input arguments (e.g., `location: "San Francisco"`).
2. The tool’s `execute` function runs and returns the result (e.g., `{ location: "San Francisco", temperature: 65, weather: "sunny" }`).
3. This result is wrapped into **Tool Parts** and streamed to the client inside `UIMessage`.

#### Tool Part Structure (on Client)

On the client side, tool calls and results appear as **typed tool parts** inside `UIMessage.parts`.
The part’s type is `tool-${toolName}`.

For example, if `toolName: 'displayWeather'`, the type is `tool-displayWeather`:

| Property    | Value                                                                       | Description                                                          |
| :---------- | :-------------------------------------------------------------------------- | :------------------------------------------------------------------- |
| `type`      | `tool-displayWeather`                                                       | Type name derived from the tool name.                                |
| `state`     | `input-streaming` / `input-available` / `output-available` / `output-error` | Tool execution state, useful for progress rendering.                 |
| `input`     | `{ location: string }`                                                      | Input generated by LLM.                                              |
| `output`    | `{ location: string, temperature: number, weather: string }`                | **Result from `execute`** (only when `state` is `output-available`). |
| `errorText` | `string`                                                                    | Error details (only when `state` is `output-error`).                 |

---

### Step 4: Pass into UI Components (Client-side Rendering: `app/page.tsx`)

On the client (using `useChat`), you render UI components based on `message.parts` type and state.

1. **Use `useChat`:** Manages streaming UI messages from the server.
2. **Iterate through `message.parts`:** Identify parts beyond text.
3. **Check Tool Part:** Use `switch (part.type)` for specific tool parts.
4. **Check State:** Show loading, final output, or error depending on `part.state`.
5. **Pass Data:** When `output-available`, the JSON result from `execute` is available at `part.output` and can be passed as props to React Components.

**Example Client UI (`app/page.tsx`)**:

```typescript
// app/page.tsx (Client Component)
'use client';
import { useChat } from '@ai-sdk/react';
import { Weather } from '@/components/weather'; // Custom UI Component

export default function Page() {
  const { messages, sendMessage, input, setInput } = useChat();

  return (
    <div>
      {messages.map(message => (
        <div key={message.id}>
          {message.parts.map((part, index) => {
            if (part.type === 'text') {
              return <span key={index}>{part.text}</span>;
            }

            if (part.type === 'tool-displayWeather') {
              switch (part.state) {
                case 'input-available':
                case 'input-streaming':
                  return <div key={index}>Loading weather info...</div>;

                case 'output-available':
                  return (
                    <div key={index}>
                      <Weather {...part.output} /> {/* Data from execute() */}
                    </div>
                  );

                case 'output-error':
                  return <div key={index}>Error: {part.errorText}</div>;

                default:
                  return null;
              }
            }
            return null;
          })}
        </div>
      ))}
      {/* ... Input form */}
    </div>
  );
}
```

---

### Summary of Required Files

| File                         | Role                     | Main Function                                                                                                      |
| :--------------------------- | :----------------------- | :----------------------------------------------------------------------------------------------------------------- |
| **`ai/tools.ts`**            | Tool Definition (Server) | Contains `tool()`, `inputSchema`, and `execute` returning JSON data.                                               |
| **`app/api/chat/route.ts`**  | Route Handler (Server)   | Accepts `UIMessage[]`, converts to `ModelMessage[]`, calls `streamText` with tools, and returns a stream response. |
| **`components/Weather.tsx`** | UI Component (Client)    | React component consuming tool output (e.g., `temperature`, `location`).                                           |
| **`app/page.tsx`**           | Chat UI (Client)         | Uses `useChat`, renders `message.parts`, and passes `part.output` to UI components.                                |
